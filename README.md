RoadRunner is a novel approach that accelerates transformer inference by eliminating expensive matrix multiplications without compromising output quality. The key discovery is that transformers contain inherent structural properties that allow bypassing the computational bottlenecks in both MLP blocks and language model (LM) heads. Through Singular Value Decomposition (SVD) of transformer weight matrices, RoadRunner creates efficient computational pathways that preserve semantic integrity while dramatically reducing total operations.

Experiments with both GPT-2 and Llama-3.2-1B reveal that transformer hidden states naturally align with target token embeddings to a remarkable degree, enabling direct dot-product token selection without requiring full vocabulary projection. By implementing layerwise alpha-blending that combines minimal contributions from routed computation paths (as low as 5%) with standard paths, the system maintains near-perfect token match and nearly identical output distributions (>0.99 cosine similarity).

The measured 1.57Ã— speedup was achieved with an unoptimized proof of concept implementation, primarily demonstrating that large matrix multiplications can be bypassed without accuracy loss. Future derivative works applying RoadRunner's technique could achieve revolutionary speed increases, particularly if the same approach is extended to the self-attention mechanism. This research opens the door to dramatically faster transformer inference with pretrained weights while maintaining near-perfect accuracy compared to baseline inference.
